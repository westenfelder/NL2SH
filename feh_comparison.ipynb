{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Equivalence Heuristic Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NL2CMD accuracy metric\n",
    "\n",
    "! rm -rf bashlint/ metric/\n",
    "! git clone https://github.com/IBM/clai.git\n",
    "! git -C ./clai checkout nlc2cmd\n",
    "! cp -r clai/utils/bashlint bashlint\n",
    "! cp -r clai/utils/metric metric\n",
    "! rm -rf clai\n",
    "! sed -i 's/import collections/import collections.abc as collections/' bashlint/butils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results folder\n",
    "\n",
    "! mkdir feh_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from metric import metric_utils\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from icalfa import submit_command\n",
    "from openai import OpenAI\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API keys\n",
    "\n",
    "os.environ['ICALFA_OPENAI_API_KEY'] = '...'\n",
    "\n",
    "client = OpenAI(api_key='...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama and OpenAI functions\n",
    "\n",
    "system_prompt = \"You will be given a task and two Bash commands. The first command is the ground truth. If the second command accomplishes the task, return true. Otherwise, return false. Only output 'true' or 'false'.\"\n",
    "\n",
    "def openai(prompt, model):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    seed=123\n",
    "    )\n",
    "    content = completion.choices[0].message.content\n",
    "    stripped = content.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "    if stripped.lower() == \"true\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def ollama(prompt, model):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    payload = json.dumps({\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0,\n",
    "        \"seed\": 123\n",
    "    })\n",
    "    response = requests.post(url, data=payload)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error creating request: {response.text}\")\n",
    "    else:\n",
    "        response_json = response.json()\n",
    "        response_content = response_json['message']['content']\n",
    "        stripped = response_content.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "        if stripped.lower() == \"true\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bleu FEH\n",
    "def bleu(index, prompt, ground_truth_command, model_command):\n",
    "    gt_list = list(ground_truth_command)\n",
    "    model_list = list(model_command)\n",
    "    bleu_score = sentence_bleu([gt_list], model_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1)\n",
    "    return bleu_score > 0.75\n",
    "\n",
    "# NL2CMD FEH\n",
    "def nl2cmd(index, prompt, ground_truth_command, model_command):\n",
    "    nl2cmd_score = metric_utils.compute_metric(predicted_cmd=model_command, predicted_confidence=1, ground_truth_cmd=ground_truth_command)\n",
    "    # scale score to [0-1]\n",
    "    nl2cmd_score = (nl2cmd_score + 1)/2\n",
    "    return nl2cmd_score > 0.75\n",
    "\n",
    "# TF-IDF FEH\n",
    "def tfidf(index, prompt, ground_truth_command, model_command):\n",
    "    try:\n",
    "        vect = TfidfVectorizer()\n",
    "        tfidf = vect.fit_transform([ground_truth_command, model_command])\n",
    "        similarity = tfidf * tfidf.T\n",
    "        tfidf_score = similarity.toarray()[0][1]\n",
    "    except:\n",
    "        tfidf_score = 0\n",
    "    return tfidf_score > 0.75\n",
    "\n",
    "# Execution + TF-IDF FEH\n",
    "def exec_tfidf(index, prompt, ground_truth_command, model_command):\n",
    "    score = submit_command(index=index, command=model_command, eval_mode=\"tfidf\")\n",
    "    return score == 1\n",
    "\n",
    "# mxbai-embed FEH\n",
    "embedding_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "def mxbai_embed(index, prompt, ground_truth_command, model_command):\n",
    "    embeddings = embedding_model.encode([ground_truth_command, model_command])\n",
    "    cosine_similarity = cos_sim(embeddings[0], embeddings[1])\n",
    "    embedding_score = cosine_similarity.item()\n",
    "    return embedding_score > 0.75\n",
    "\n",
    "# Execution + mxbai-embed FEH\n",
    "def exec_mxbai_embed(index, prompt, ground_truth_command, model_command):\n",
    "    score = submit_command(index=index, command=model_command, eval_mode=\"embed\", eval_param=0.75)\n",
    "    return score == 1\n",
    "\n",
    "# Llama3.1-8b-Instruct FEH\n",
    "def llama3(index, prompt, ground_truth_command, model_command):\n",
    "    score = ollama(f\"Task: {prompt}, Ground Truth Command: {ground_truth_command}, Model Command: {model_command}\", \"llama3.1:8b\")\n",
    "    return score == 1\n",
    "\n",
    "# Execution + Llama3.1-8b-Instruct FEH\n",
    "def exec_llama3(index, prompt, ground_truth_command, model_command):\n",
    "    score = submit_command(index=index, command=model_command, eval_mode=\"ollama\", eval_param=\"llama3.1:8b\")\n",
    "    return score == 1\n",
    "\n",
    "# GPT-3.5-Turbo FEH\n",
    "def gpt3(index, prompt, ground_truth_command, model_command):\n",
    "    score = openai(f\"Task: {prompt}, Ground Truth Command: {ground_truth_command}, Model Command: {model_command}\", \"gpt-3.5-turbo-0125\")\n",
    "    return score == 1\n",
    "\n",
    "# Execution + GPT-3.5-Turbo FEH\n",
    "def exec_gpt3(index, prompt, ground_truth_command, model_command):\n",
    "    score = submit_command(index=index, command=model_command, eval_mode=\"openai\", eval_param=\"gpt-3.5-turbo-0125\")\n",
    "    return score == 1\n",
    "\n",
    "# GPT-4 FEH\n",
    "def gpt4(index, prompt, ground_truth_command, model_command):\n",
    "    score = openai(f\"Task: {prompt}, Ground Truth Command: {ground_truth_command}, Model Command: {model_command}\", \"gpt-4-0613\")\n",
    "    return score == 1\n",
    "\n",
    "# Execution + GPT-4 FEH\n",
    "def exec_gpt4(index, prompt, ground_truth_command, model_command):\n",
    "    score = submit_command(index=index, command=model_command, eval_mode=\"openai\", eval_param=\"gpt-4-0613\")\n",
    "    return score == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['bleu.csv', 'nl2cmd.csv', 'tfidf.csv', 'exec_tfidf.csv', 'mxbai_embed.csv', 'exec_mxbai_embed.csv', 'llama3.csv', 'exec_llama3.csv', 'gpt3.csv', 'exec_gpt3.csv', 'gpt4.csv', 'exec_gpt4.csv']\n",
    "\n",
    "fehs = [bleu, nl2cmd, tfidf, exec_tfidf, mxbai_embed, exec_mxbai_embed, llama3, exec_llama3, gpt3, exec_gpt3, gpt4, exec_gpt4]\n",
    "\n",
    "dataset = load_dataset(\"westenfelder/NL2SH-ALFA\", \"test\", split=\"train\")\n",
    "\n",
    "# rotate dataset by 10 positions to create non-equivalent commands\n",
    "indices = list(range(300))\n",
    "shuffled = indices[-10:] + indices[:-10]\n",
    "\n",
    "for file_index, name in enumerate(file_names):\n",
    "    if os.path.exists(f\"feh_results/{name}\"):\n",
    "        print(f\"{name} already exists, skipping\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Creating {name}\")\n",
    "\n",
    "    with open(f\"feh_results/{name}\", mode='w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Task', 'Ground Truth Command', 'Model Command', 'Functional Equivalence', 'Heuristic Output'])\n",
    "\n",
    "        for data_index, row in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "            prompt = row['nl']\n",
    "            ground_truth_command = row['bash']\n",
    "            func_equiv_command = row['bash2']\n",
    "            not_func_equiv_command = dataset[shuffled[data_index]][\"bash\"]\n",
    "            # get heuristic output\n",
    "            func_equiv_feh_output = fehs[file_index](data_index, prompt, ground_truth_command, func_equiv_command)\n",
    "            not_func_equiv_feh_output = fehs[file_index](data_index, prompt, ground_truth_command, not_func_equiv_command)\n",
    "\n",
    "            writer.writerow([prompt, ground_truth_command, func_equiv_command, \"True\", func_equiv_feh_output])\n",
    "            writer.writerow([prompt, ground_truth_command, not_func_equiv_command, \"False\", not_func_equiv_feh_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "\n",
    "results = [[\"Heuristic\", \"TP\", \"FP\", \"TN\", \"FN\", \"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"]]\n",
    "for name in file_names:        \n",
    "    with open(f\"feh_results/{name}\", mode='r') as file:\n",
    "        read = csv.reader(file)\n",
    "\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        true_negative = 0\n",
    "        false_negative = 0\n",
    "        for row in read:\n",
    "            if row[3] == \"True\" and row[4] == \"True\":\n",
    "                true_positive += 1\n",
    "            elif row[3] == \"True\" and row[4] == \"False\":\n",
    "                false_negative += 1\n",
    "            elif row[3] == \"False\" and row[4] == \"True\":\n",
    "                false_positive += 1\n",
    "            elif row[3] == \"False\" and row[4] == \"False\":\n",
    "                true_negative += 1\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        acc = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "        \n",
    "        results.append([name, true_positive, false_positive, true_negative, false_negative, f\"{precision:0.2f}\", f\"{recall:0.2f}\", f\"{f1:0.2f}\", f\"{acc:0.2f}\"])\n",
    "\n",
    "latex_table = tabulate(results, headers=\"firstrow\", tablefmt=\"latex\")\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nl2sh_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
