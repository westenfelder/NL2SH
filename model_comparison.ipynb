{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results folder\n",
    "\n",
    "! mkdir model_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import csv\n",
    "from icalfa import submit_command\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names\n",
    "\n",
    "models = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Coder-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Coder-3B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    \"gpt-3.5-turbo-0125\",\n",
    "    \"gpt-4o-mini-2024-07-18\",\n",
    "    \"gpt-4o-2024-08-06\",\n",
    "    \"gpt-4-0613\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI function\n",
    "\n",
    "client = OpenAI(api_key='...')\n",
    "\n",
    "def openai_completion(prompt, system_prompt, model, tokenizer):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    seed=123\n",
    "    )\n",
    "    content = completion.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers function\n",
    "\n",
    "def transformers_completion(prompt, system_prompt, model, tokenizer):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    attention_mask = torch.ones_like(tokens)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    if model.__class__.__name__.startswith(\"Qwen\"):\n",
    "        outputs = model.generate(\n",
    "            tokens,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "        )\n",
    "    else:\n",
    "        outputs = model.generate(\n",
    "            tokens,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=100,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "        )\n",
    "    \n",
    "    response = outputs[0][tokens.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip markdown formatting\n",
    "\n",
    "def parse_bash(text):\n",
    "    patterns = [\n",
    "        r\"```bash\\s*(.*?)\\s*```\",\n",
    "        r\"```(.*?)```\",\n",
    "        r\"`(.*?)`\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark function\n",
    "\n",
    "def benchmark(model_name):\n",
    "    filename = model_name.split(\"/\")[-1]\n",
    "    results_file = f\"model_results/{filename}.csv\"\n",
    "    if os.path.exists(results_file):\n",
    "        print(f\"{results_file} already exists, skipping\")\n",
    "\n",
    "    else:\n",
    "        system_prompt = \"Your task is to translate a natural language instruction to a Bash command. You will receive an instruction in English and output a Bash command that can be run in a Linux terminal.\"\n",
    "\n",
    "        if model_name.startswith(\"gpt\"):\n",
    "            get_completion = openai_completion\n",
    "            model = model_name\n",
    "            tokenizer = None\n",
    "        else:\n",
    "            get_completion = transformers_completion\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "        dataset = load_dataset(\"westenfelder/NL2SH-ALFA\", \"test\", split=\"train\")\n",
    "\n",
    "        with open(results_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['prompt', 'ground_truth_command', 'model_command', 'correct'])\n",
    "\n",
    "            for index, row in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "                prompt = row['nl']\n",
    "                ground_truth_command = row['bash']\n",
    "                response = get_completion(prompt=prompt, system_prompt=system_prompt, model=model, tokenizer=tokenizer)\n",
    "                model_command = parse_bash(response)\n",
    "                correct = submit_command(index=index, command=model_command, eval_mode=\"embed\", eval_param=0.75)\n",
    "                writer.writerow([prompt, ground_truth_command, model_command, correct])\n",
    "\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark models\n",
    "\n",
    "for model_name in models:\n",
    "    benchmark(model_name)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "\n",
    "results = [[\"Model\", \"Accuracy\"]]\n",
    "for model_name in models:\n",
    "    filename = model_name.split(\"/\")[-1]\n",
    "    results_file = f\"model_results/{filename}.csv\"\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, mode='r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            correct = 0\n",
    "            for row in reader:\n",
    "                if row[3] == '1':\n",
    "                    correct += 1\n",
    "            results.append([model_name, f\"{(correct/300):0.2f}\"])\n",
    "            file.close()\n",
    "\n",
    "latex_table = tabulate(results, headers=\"firstrow\", tablefmt=\"latex\")\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nl2sh_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
